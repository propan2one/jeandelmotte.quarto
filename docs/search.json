[
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Home",
    "section": "",
    "text": "Hello, I’m Jean DELMOTTE immunologist by training, bioinformatician by profession, educator and scientist at heart. At the interface of data science, biology, and, computer science."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Coming from a multidisciplinary background mixing biochemistry, microbiology, immunology and bioinformatics, I’m now a Bioinformatician researcher at Sanofi. My work focuses on creating complex bioinformatics analysis for the pharmaceutical industry.\nIn this blog I study the scientific fields that interest me (pangenomics, data workflows and bioinformatics pipelines..). I also talk about my feelings in the pharmaceutical environment after my career in academia, and I express opinions that are my own and commit only myself."
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "The different generation of bioinformatics analysis",
    "section": "",
    "text": "Today, the volume of data to be analysed in bioinformatics is such that it’s essential to use a number of tools to produce results in a consistent timeframe. However, these advances are not always used, so let’s take a look back at the history of analysis methods over the last 10 years.\n\n\nTo fully understand what these modalities refer to, II need to introduce you to the notion that I call the different generations, often speaking of the Generation I, II or III pipeline (abbreviated as GI, GII etc..). Of course, defined limits are not so clearly defined, and often generations of tools coexist within a lab or organization.\n\nGeneration 0\nLet’s start with an era I’ve never known (I’ve only been doing bioinfo for 7 years), which includes the beginnings of bioinformatics. Here I call the tools used generation 0."
  },
  {
    "objectID": "contact.html",
    "href": "contact.html",
    "title": "Contact",
    "section": "",
    "text": "If you have any problem that I can help you with, I would be glad to help!\nThe best way to reach me is to send me a message on Mastodon or twitter/X"
  },
  {
    "objectID": "homing.html",
    "href": "homing.html",
    "title": "About",
    "section": "",
    "text": "Hello, I am Jean DELMOTTE biologist by training, computational biologist by profession, educator and scientist at heart. At the interface of data science, biology, and, computer science."
  },
  {
    "objectID": "homepage.html",
    "href": "homepage.html",
    "title": "Posts",
    "section": "",
    "text": "Synthetic data generation\n\n\n\n\n\n\nBioinformatics\n\n\nSynthetic\n\n\nPython\n\n\n\n\n\n\n\n\n\nDec 27, 2024\n\n\nJean DELMOTTE\n\n\n\n\n\n\n\n\n\n\n\n\nThe different generation of bioinformatics analysis\n\n\n\n\n\n\nOpinion\n\n\nBioinformatics\n\n\nWorkflow managers\n\n\n\n\n\n\n\n\n\nNov 21, 2024\n\n\nJean DELMOTTE\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "docs/main.html",
    "href": "docs/main.html",
    "title": "About",
    "section": "",
    "text": "Hello, I am Jean DELMOTTE biologist by training, computational biologist by profession, educator and scientist at heart. At the interface of data science, biology, and, computer science."
  },
  {
    "objectID": "contact.html#contact-me",
    "href": "contact.html#contact-me",
    "title": "Contact",
    "section": "",
    "text": "If you have any problem that I can help you with, I would be glad to help"
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "About",
    "section": "Education",
    "text": "Education\n\nUniversity of Montpellier, Montpellier | France, Fr\nPh.D in Bioinformatics | Oct. 2017 - Apr. 2021\nUniversité de Paris, Paris | France, Fr\nMaster degree in Immunology Sep. 2015 - Jun. 2017\nUniversité de Paris, Paris | France, Fr\nB.S. degree in Biochemistry Bioinformatics Biology Sep. 2014 - Jun. 2015\nUniversity of Toulon, Toulon | France, Fr University degree in technology Sep. 2012 - Jun. 2014"
  },
  {
    "objectID": "posts/post-with-code/index.html#context",
    "href": "posts/post-with-code/index.html#context",
    "title": "The evolution of bioinformatics analysis workflows",
    "section": "",
    "text": "Today, the volume of data to be analysed in bioinformatics is such that it’ss essential to use a number of tools to produce results in a consistent timeframe. However, these advances are not always used, so let’s take a look back at the history of analysis methods over the last 10 years.\n\n\nTo fully understand what these modalities refer to, I need to introduce the notion of generation. Of course, defined limits are not so clearly defined, and often generations of tools coexist within a lab or organization.\n\n\nLet’s start with a period I didn’t experience, which includes the beginnings of bioinformatics. Here I call the tools used generation 0"
  },
  {
    "objectID": "posts/generation-bioinformatics/index.html",
    "href": "posts/generation-bioinformatics/index.html",
    "title": "The different generation of bioinformatics analysis",
    "section": "",
    "text": "Today, the volume of data to be analysed in bioinformatics is such that it’s essential to use a number of tools to produce results in a consistent timeframe. However, these advances are not always used, so let’s take a look back at the history of analysis methods over the last 10 years.\n\n\nTo fully understand what these modalities refer to, II need to introduce you to the notion that I call the different generations, often speaking of the Generation I, II or III pipeline (abbreviated as GI, GII etc..). Of course, defined limits are not so clearly defined, and often generations of tools coexist within a lab or organization.\n\nGeneration 0\nLet’s start with an era I’ve never known (I’ve only been doing bioinfo for 7 years), which includes the beginnings of bioinformatics. Here I call the tools used generation 0."
  },
  {
    "objectID": "posts/bioinformatics-generation/index.html",
    "href": "posts/bioinformatics-generation/index.html",
    "title": "The different generation of bioinformatics analysis",
    "section": "",
    "text": "Today, the volume of data to be analysed in bioinformatics is such that it’s essential to use a number of tools to produce results in a consistent timeframe. However, these advances are not always used, so let’s take a look back at the history of analysis methods over the last 10 years.\n\n\nTo fully understand what these modalities refer to, I need to introduce you to the notion that I call the different generations, often speaking of the generation I, II or III pipeline (abbreviated as GI, GII etc..). Of course, defined limits are not so clearly defined, and often generations of tools coexist within a lab or organization. I’ll now describe what I consider to be the different generations, and then conclude by explaining the advantages of such a category. If you’re a bioinformatician, you can go straight to the GII pipeline.\n\nGeneration I pipelines\nFunnily enough, these different generations can be found throughout a bioinformatician’s career. Because I’ve been doing bioinfo for 7 years I started my journey by using Graphical tools. There are many solutions for processing data using a grapichal user interface, such as Ugene a free open-source bioinformatics software, Geneious prime, CLC genomics workbench or even snapgene.\nThe advantage of this software is that it allows you to quickly understand the data you’re working with. This is very important for genomic analyses, for example, where data can be easily trimmed, mapped against a reference or even perform de novo assembly. The visualizations of data or results they provide are very interesting. However, for some of them, the algorithms used are black boxes such as trimming or variant calling in CLC. So the main risk, which is also their strength, is that they try to summarize complex parameters in simple values. This type of approach may be sufficient as long as complex cases are not encountered, and pharmaceutical companies love simple answers. Another concern for users is the manual, sequential (one sample at a time) set-up of the analysis. Some software packages allow batch analysis, which is good but not efficient.\nTogether, Generation I (GI) tools/pipelines are sufficient for small datasets, often for use by biologists. They are used routinely, with a very simple workflow. Bioinformaticians employing them can use them to visualize data or decrease the workload by training biologists. Sometimes by coding a small part on the side (merging VCFs etc.).\n\n\nGeneration II pipelines\n\n\nWhat I call Generation II (GII) pipelines are everything that comes close to bash-coded pipelines. When coupled with a launch interface and an academic or cloud server, it allows you to run all available bioinfo tools. We’re not going to lie, we’ve all done it, it’s ugly, but it works and sometimes it allows you to go fast (for a POC or whatever). Sometimes it’s an analysis in Python or R, but the idea remains the same: it’s a script (often monolithic) that chains together the analysis tools (Fig.1.). You’ll often find conda environment calls directly in the code or singularity image to manage the bioinformatics sofware. Most of the time, it’s the bioinformatician who developed it who launches it for his analyses, but sometimes with the use of an app (a feature of GIII) allows the pipeline to be transmitted to several users, including biologists. Here the constraint is to provide the user with a consistent interface (asking the user to modify a YAML can sometimes be problematic for the user).\n\n\n\n\n\n\nFig.1. One script to rule them all\n\n\n\n\nThrough these GII pipelines, bioinformaticians are free to use all the tools developed by the community or themselves, the only limit being computing power. Then, in the case of a large number of samples, a loop to launch the command line parsing a TSV file retrieving the raw data will do the trick (which is built using good old ls of course). All this is feasible and, depending on the need, it’s not a bad thing. After all, it’s the same manual analysis we do on our servers. I know of production pipelines that are still running with this several years after the last commit, and they’re reliable! Just to mention it, using Galaxy is a bit of a hybrid between GI & GII, as it provides a wide range of up-to-date tools via its community, the possibility of assembling them into DAGs and automating complex analyses.\nThe limitation of these generation two pipelines is that i) since the analyses are sequential, the computational cost is proportional to the number of samples. ii) Often it’s a combination of generation II pipeline to make end to end analysis. For example, a pipeline run ‘n’ times to assemble ‘n’ genomes, and another pipeline to compare these ‘n’ genomes. iii) Working with several people on these tools can quickly become a nightmare.\n\n\nGeneration III\nThat’s where Generation III pipelines come in by using the Workflow Management Systems (WfMS) (Ahmed et al. 2021). From my point of view, it’s literally a language for parallelizing task. In the case of bioinformatics analysis it’s very powerful because you sahre the computing power between samples and process, can trigger several comportement for autoscaling pipeline and more. You’re probably familiar with WfMS languages such as Snakemake, Nextflow, CWL, Airflow and WDL etc..\nWhen coupled with a launch interface (ShinyApp, Streamlit or even better an EPAM Cloud Pipeline infra) and an academic or cloud server, it allows you to run all available bioinfo tools.\nTo be continue…"
  },
  {
    "objectID": "posts/synthetic-data/index.html",
    "href": "posts/synthetic-data/index.html",
    "title": "Synthetic data generation",
    "section": "",
    "text": "Image from TheOrsna in Pixabay\n\n\n\n\n\nOne of the most cost-effective things bioinformatics can do is to simulate giving. These simulations are an underestimated research tool. We often simulate data to benchmark different algorithms or to perform unit tests on our analysis pipelines. From my personal point of view, it’s possible to go further: if we understand a biological problem, then we should be able to simulate data. If the entire problem is correctly understood, then the analysis of simulated and experimental data should lead to the same conclusion. This validates that the mechanism is well understood.\n\n\nIn this post, I’ll describe how to create a small synthetic genomes, assign mutations to it and simulate mutations at different frequencies. The aim is to create a minimal dataset to facilitate unit testing of my pipelines.\n\nCreation of a synthetic genomes\nThere are many ways of doing this. When working on a single model, it’s best to start with the genome of interest and introduce mutations. If, on the other hand, you want something completly artificial, you can generate a sequence with a probability of 0.25 for each letter (A, T, C and G). A lot of tools exist, like random_dna from Sequence Manipulation Suit, all bioinformatics libraries, makenucseq from EMBOSS.. But a long time ago I made a very basic package inSilicoGenome which allows you to generate sequences and mutations, I’m going to use it. As this project never came to a conclusion, I’m going to take this opportunity to show you what not to do, while still managing to get the code to work without using the pip command.\n\nGet the code first, install a correct envs in a linux terminal having conda/mamba\n\n# 1) Clone the repo \ngit clone git@github.com:propan2one/inSilicoGenome.git\n \n# 2) create a conda env where all tools work together\n#    in my case, for ease of use, I'm going to use biopython\nconda create -y -p ~/envs/insilicogenome \\\n    --channel conda-forge python=3.11.11 biopython \nconda activate ~/envs/insilicogenome\n\nThen use python’s dedicated functions to create the sequences corresponding to the different haplotypes\n\n# 3) Import function available in the package\nimport sys\nsys.path.append('inSilicoGenome')\nfrom src.insilicogenome import insilicogenome\nfrom src.insilicogenome import insilicodata\n\n# 4) Declare all variables\nsize = 5000\noutput = \"haplo_01.fasta\"\nrange_start=10\nrange_end=4500\n\n# 5) Make a reference sequence\nsequence = insilicogenome.random_dnasequence(size)\ninsilicogenome.write_fasta_genome(output, sequence, description = '0.8')\n\n# 6) Make a haplotype\nvcf = insilicodata.generate_table_small_variation(output, range_start=range_start, range_end=range_end)\ninsilicodata.create_variants(output, vcf, range_start=range_start, range_end=range_end)\nOnce you’ve created the second haplotype, you should have a new fasta sequence and a pseudo VCF that keeps track of all the variations. So with the generate_table_small_variation function you’ll have, taking the reference: a small insertion and deletion of one base, an insertion and deletion of 5 bases, 1 SNV and 1 MNV broken down into 5 SNVs making a total of 6 variations.\n\n\nSimulation of Illumina data for both haplotypes\nTo be continue…"
  }
]