[
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Home",
    "section": "",
    "text": "Hello, I’m Jean DELMOTTE immunologist by training, bioinformatician by profession, educator and scientist at heart. At the interface of data science, biology, and, computer science."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Coming from a multidisciplinary background mixing biochemistry, microbiology, immunology and bioinformatics, I’m now a Bioinformatician researcher at Sanofi. My work focuses on creating complex bioinformatics analysis for the pharmaceutical industry.\nIn this blog I study the scientific fields that interest me (pangenomics, data workflows and bioinformatics pipelines..). I also talk about my feelings in the pharmaceutical environment after my career in academia, and I express opinions that are my own and commit only myself."
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "The different generation of bioinformatics analysis",
    "section": "",
    "text": "Today, the volume of data to be analysed in bioinformatics is such that it’s essential to use a number of tools to produce results in a consistent timeframe. However, these advances are not always used, so let’s take a look back at the history of analysis methods over the last 10 years.\n\n\nTo fully understand what these modalities refer to, II need to introduce you to the notion that I call the different generations, often speaking of the Generation I, II or III pipeline (abbreviated as GI, GII etc..). Of course, defined limits are not so clearly defined, and often generations of tools coexist within a lab or organization.\n\nGeneration 0\nLet’s start with an era I’ve never known (I’ve only been doing bioinfo for 7 years), which includes the beginnings of bioinformatics. Here I call the tools used generation 0."
  },
  {
    "objectID": "contact.html",
    "href": "contact.html",
    "title": "Contact",
    "section": "",
    "text": "If you have any problem that I can help you with, I would be glad to help!\nThe best way to reach me is to send me a message on Mastodon or twitter/X"
  },
  {
    "objectID": "homing.html",
    "href": "homing.html",
    "title": "About",
    "section": "",
    "text": "Hello, I am Jean DELMOTTE biologist by training, computational biologist by profession, educator and scientist at heart. At the interface of data science, biology, and, computer science."
  },
  {
    "objectID": "homepage.html",
    "href": "homepage.html",
    "title": "Posts",
    "section": "",
    "text": "Synthetic data generation\n\n\n\n\n\n\nBioinformatics\n\n\nSynthetic\n\n\nSimulation\n\n\nPython\n\n\n\n\n\n\n\n\n\nDec 27, 2024\n\n\nJean DELMOTTE\n\n\n\n\n\n\n\n\n\n\n\n\nThe different generation of bioinformatics analysis\n\n\n\n\n\n\nOpinion\n\n\nBioinformatics\n\n\nWorkflow managers\n\n\n\n\n\n\n\n\n\nNov 21, 2024\n\n\nJean DELMOTTE\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "docs/main.html",
    "href": "docs/main.html",
    "title": "About",
    "section": "",
    "text": "Hello, I am Jean DELMOTTE biologist by training, computational biologist by profession, educator and scientist at heart. At the interface of data science, biology, and, computer science."
  },
  {
    "objectID": "contact.html#contact-me",
    "href": "contact.html#contact-me",
    "title": "Contact",
    "section": "",
    "text": "If you have any problem that I can help you with, I would be glad to help"
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "About",
    "section": "Education",
    "text": "Education\n\nUniversity of Montpellier, Montpellier | France, Fr\nPh.D in Bioinformatics | Oct. 2017 - Apr. 2021\nUniversité de Paris, Paris | France, Fr\nMaster degree in Immunology Sep. 2015 - Jun. 2017\nUniversité de Paris, Paris | France, Fr\nB.S. degree in Biochemistry Bioinformatics Biology Sep. 2014 - Jun. 2015\nUniversity of Toulon, Toulon | France, Fr University degree in technology Sep. 2012 - Jun. 2014"
  },
  {
    "objectID": "posts/post-with-code/index.html#context",
    "href": "posts/post-with-code/index.html#context",
    "title": "The evolution of bioinformatics analysis workflows",
    "section": "",
    "text": "Today, the volume of data to be analysed in bioinformatics is such that it’ss essential to use a number of tools to produce results in a consistent timeframe. However, these advances are not always used, so let’s take a look back at the history of analysis methods over the last 10 years.\n\n\nTo fully understand what these modalities refer to, I need to introduce the notion of generation. Of course, defined limits are not so clearly defined, and often generations of tools coexist within a lab or organization.\n\n\nLet’s start with a period I didn’t experience, which includes the beginnings of bioinformatics. Here I call the tools used generation 0"
  },
  {
    "objectID": "posts/generation-bioinformatics/index.html",
    "href": "posts/generation-bioinformatics/index.html",
    "title": "The different generation of bioinformatics analysis",
    "section": "",
    "text": "Today, the volume of data to be analysed in bioinformatics is such that it’s essential to use a number of tools to produce results in a consistent timeframe. However, these advances are not always used, so let’s take a look back at the history of analysis methods over the last 10 years.\n\n\nTo fully understand what these modalities refer to, II need to introduce you to the notion that I call the different generations, often speaking of the Generation I, II or III pipeline (abbreviated as GI, GII etc..). Of course, defined limits are not so clearly defined, and often generations of tools coexist within a lab or organization.\n\nGeneration 0\nLet’s start with an era I’ve never known (I’ve only been doing bioinfo for 7 years), which includes the beginnings of bioinformatics. Here I call the tools used generation 0."
  },
  {
    "objectID": "posts/bioinformatics-generation/index.html",
    "href": "posts/bioinformatics-generation/index.html",
    "title": "The different generation of bioinformatics analysis",
    "section": "",
    "text": "Today, the volume of data to be analysed in bioinformatics is such that it’s essential to use a number of tools to produce results in a consistent timeframe. However, these advances are not always used, so let’s take a look back at the history of analysis methods over the last 10 years.\n\n\nTo fully understand what these modalities refer to, I need to introduce you to the notion that I call the different generations, often speaking of the generation I, II or III pipeline (abbreviated as GI, GII etc..). Of course, defined limits are not so clearly defined, and often generations of tools coexist within a lab or organization. I’ll now describe what I consider to be the different generations, and then conclude by explaining the advantages of such a category. If you’re a bioinformatician, you can go straight to the GII pipeline.\n\nGeneration I pipelines\nFunnily enough, these different generations can be found throughout a bioinformatician’s career. Because I’ve been doing bioinfo for 7 years I started my journey by using Graphical tools. There are many solutions for processing data using a grapichal user interface, such as Ugene a free open-source bioinformatics software, Geneious prime, CLC genomics workbench or even snapgene.\nThe advantage of this software is that it allows you to quickly understand the data you’re working with. This is very important for genomic analyses, for example, where data can be easily trimmed, mapped against a reference or even perform de novo assembly. The visualizations of data or results they provide are very interesting. However, for some of them, the algorithms used are black boxes such as trimming or variant calling in CLC. So the main risk, which is also their strength, is that they try to summarize complex parameters in simple values. This type of approach may be sufficient as long as complex cases are not encountered, and pharmaceutical companies love simple answers. Another concern for users is the manual, sequential (one sample at a time) set-up of the analysis. Some software packages allow batch analysis, which is good but not efficient.\nTogether, Generation I (GI) tools/pipelines are sufficient for small datasets, often for use by biologists. They are used routinely, with a very simple workflow. Bioinformaticians employing them can use them to visualize data or decrease the workload by training biologists. Sometimes by coding a small part on the side (merging VCFs etc.).\n\n\nGeneration II pipelines\n\n\nWhat I call Generation II (GII) pipelines are everything that comes close to bash-coded pipelines. When coupled with a launch interface and an academic or cloud server, it allows you to run all available bioinfo tools. We’re not going to lie, we’ve all done it, it’s ugly, but it works and sometimes it allows you to go fast (for a POC or whatever). Sometimes it’s an analysis in Python or R, but the idea remains the same: it’s a script (often monolithic) that chains together the analysis tools (Fig.1.). You’ll often find conda environment calls directly in the code or singularity image to manage the bioinformatics sofware. Most of the time, it’s the bioinformatician who developed it who launches it for his analyses, but sometimes with the use of an app (a feature of GIII) allows the pipeline to be transmitted to several users, including biologists. Here the constraint is to provide the user with a consistent interface (asking the user to modify a YAML can sometimes be problematic for the user).\n\n\n\n\n\n\nFig.1. One script to rule them all\n\n\n\n\nThrough these GII pipelines, bioinformaticians are free to use all the tools developed by the community or themselves, the only limit being computing power. Then, in the case of a large number of samples, a loop to launch the command line parsing a TSV file retrieving the raw data will do the trick (which is built using good old ls of course). All this is feasible and, depending on the need, it’s not a bad thing. After all, it’s the same manual analysis we do on our servers. I know of production pipelines that are still running with this several years after the last commit, and they’re reliable! Just to mention it, using Galaxy is a bit of a hybrid between GI & GII, as it provides a wide range of up-to-date tools via its community, the possibility of assembling them into DAGs and automating complex analyses.\nThe limitation of these generation two pipelines is that i) since the analyses are sequential, the computational cost is proportional to the number of samples. ii) Often it’s a combination of generation II pipeline to make end to end analysis. For example, a pipeline run ‘n’ times to assemble ‘n’ genomes, and another pipeline to compare these ‘n’ genomes. iii) Working with several people on these tools can quickly become a nightmare.\n\n\nGeneration III\nThat’s where Generation III pipelines come in by using the Workflow Management Systems (WfMS) ((Ahmed et al. 2021)). From my point of view, it’s literally a language for parallelizing task. The aim is to launch groups of samples, and the more samples are launched, the lower the cost per sample. In addition to optimizing the use of computing resources, GIIIs pipelines enable inter-sample analysis to be carried out, resulting in the production of direct downstream analyses. To be able to execute these tasks in parallel domain-specific languages (DSLs) must be used (contrary for example to a general-purpose language (GPL) like Python). The two most widely used in bioinformatics are Nextflow ((Ewels et al. 2020)) and Snakemake ((Mölder et al. 2021)), but there are many others (notably CWL, Airflow and WDL etc.. ).\n\n\n\nDefinition of pipeline logic based on modular process declarations (DAG?).\n\nThis makes the code more readable and maintainable.\n\n\n\n\nAggregate multiple samples\n\nInter-sample analysis -&gt; Automatic downstream analysis\n\n\n\n\n\nData driven execution based\n\nRef or without ref (scenario?)\n\n\n\n\nConditional execution based on data re-evaluation\n\nCheckpoint\n\n\n\n\n\nCombinatorial execution\n\nMultiple tools in parallele\n\n\n\n\nCloud and High-Performance Computing (HPC) clusters compliance\n\nScalability and Automation\n\n\n\n\n\nIntegration for package managers & containerization platforms\n\nportability with Conda, Docker, singularity\n\n\n\n\nContinuous integration testing and code-quality lint tests\n\nTemplate?\n\n\nWhen coupled with a launch interface (ShinyApp, Streamlit or even better an EPAM Cloud Pipeline infra) and an academic or cloud server, it allows you to run all available bioinfo tools.\n. In the case of bioinformatics analysis it’s very powerful because you share the computing power between samples and process, can trigger several comportement for autoscaling pipeline and more.\nLater on in this blog, I hope to show you how the different features of each of these WfMS languages are coded, so that you can compare them.\nTo be continue…\n\n\nReferences\n\n\nAhmed, Azza E., Joshua M. Allen, Tajesvi Bhat, Prakruthi Burra, Christina E. Fliege, Steven N. Hart, Jacob R. Heldenbrand, et al. 2021. “Design Considerations for Workflow Management Systems Use in Production Genomics Research and the Clinic.” Scientific Reports 11 (1): 21680. https://doi.org/10.1038/s41598-021-99288-8.\n\n\nEwels, Philip A., Alexander Peltzer, Sven Fillinger, Harshil Patel, Johannes Alneberg, Andreas Wilm, Maxime Ulysse Garcia, Paolo Di Tommaso, and Sven Nahnsen. 2020. “The Nf-Core Framework for Community-Curated Bioinformatics Pipelines.” Nature Biotechnology 38 (3): 276–78. https://doi.org/10.1038/s41587-020-0439-x.\n\n\nMölder, Felix, Kim Philipp Jablonski, Brice Letcher, Michael B. Hall, Christopher H. Tomkins-Tinch, Vanessa Sochat, Jan Forster, et al. 2021. “Sustainable Data Analysis with Snakemake,” January. https://doi.org/10.12688/f1000research.29032.1."
  },
  {
    "objectID": "posts/synthetic-data/index.html",
    "href": "posts/synthetic-data/index.html",
    "title": "Synthetic data generation",
    "section": "",
    "text": "Image from TheOrsna in Pixabay\n\n\n\n\n\nOne of the most cost-effective things bioinformatics can do is to simulate giving. These simulations are an underestimated research tool. We often simulate data to benchmark different algorithms or to perform unit tests on our analysis pipelines. From my personal point of view, it’s possible to go further: if we understand a biological problem, then we should be able to simulate data. If the entire problem is correctly understood, then the analysis of simulated and experimental data should lead to the same conclusion. This validates that the mechanism is well understood.\nIn this post, I’ll describe how to create a small synthetic genomes, assign mutations to it and simulate mutations at different frequencies. The aim is to create a minimal dataset to facilitate unit testing of my pipelines."
  },
  {
    "objectID": "posts/synthetic-data/index.html#what-did-we-learn-in-the-process",
    "href": "posts/synthetic-data/index.html#what-did-we-learn-in-the-process",
    "title": "Synthetic data generation",
    "section": "What did we learn in the process?",
    "text": "What did we learn in the process?\n\nInsert genomic variations into a reference while keeping track of them (VCF & FASTA file).\nCompare two sequences to verify the presence of the introduced variations.\nSimulate illumina data to analyze the synthetic reference."
  }
]